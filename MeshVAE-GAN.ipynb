{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e432774b-6485-4086-83cf-02764af43311",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Обновить на гитхабе этот ноутбук когда доучится"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3bee0-8646-4579-b5ac-aebf5e7af5b4",
   "metadata": {},
   "source": [
    "# Обучение MeshVAE!\n",
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e7e920-742c-4641-9f6a-ef85c44c3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "from torchvision.transforms import Resize\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch3d.renderer import look_at_view_transform\n",
    "from pytorch3d.renderer import (\n",
    "FoVPerspectiveCameras, VolumeRenderer,\n",
    "NDCGridRaysampler, EmissionAbsorptionRaymarcher\n",
    ")\n",
    "\n",
    "from modelnet import ModelNetRendersNoLabels\n",
    "\n",
    "from model_collection.MeshVAE import MeshVAE\n",
    "from model_collection.utils import huber_loss, max_VRAM_optim_fit\n",
    "from model_collection.VAE_loss.VaeLoss import VAELossGAN\n",
    "from model_collection.VAE_loss.vgg19 import VGG19\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc516114-11a9-468b-a670-5d97c8e207a3",
   "metadata": {},
   "source": [
    "## Определение девайса, датасета и даталоадера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3725d17-087c-408a-91ae-d7a18d6f3242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Test set has 10 classes\n",
      "Train set has 10 classes\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "device = 'cuda' if torch.cuda.is_available() and USE_CUDA else 'cpu'\n",
    "print('Using', device)\n",
    "\n",
    "data_sheet = pd.read_csv('./modelnet_renders_metadata.csv')\n",
    "data_train = data_sheet[data_sheet['split'] == ' train']\n",
    "data_test = data_sheet[data_sheet['split'] == ' test']\n",
    "\n",
    "image_size = 64\n",
    "transform = transforms.Compose([transforms.Resize(image_size),\n",
    "                                transforms.CenterCrop(image_size),\n",
    "                                transforms.RandomHorizontalFlip(0.5),\n",
    "                                transforms.Normalize(0.5, 0.5)])\n",
    "\n",
    "dataset_train = ModelNetRendersNoLabels(dataset_root_dir=Path('/home/student/work/3d_Generative/model/modelNet_renders'),data_sheet=data_train, transform=transform, device=device)\n",
    "dataset_test = ModelNetRendersNoLabels(dataset_root_dir=Path('/home/student/work/3d_Generative/model/modelNet_renders'),data_sheet=data_test, transform=transform, device=device)\n",
    "\n",
    "print(f\"Test set has {len(data_test['class'].unique())} classes\\nTrain set has {len(data_train['class'].unique())} classes\")\n",
    "\n",
    "data_overfit = pd.DataFrame([data_train.iloc[0], data_train.iloc[0]]) # , data_test.iloc[450]\n",
    "dataset_overfit = ModelNetRendersNoLabels(dataset_root_dir=Path('/home/student/work/3d_Generative/model/modelNet_renders'),data_sheet=data_overfit, transform=None, device=device)\n",
    "overfit_dl = torch.utils.data.DataLoader(dataset = dataset_overfit,\n",
    "                                                      batch_size = 2,\n",
    "                                                      num_workers = 0,\n",
    "                                                      shuffle = True)\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(dataset = dataset_train,\n",
    "                                                      batch_size = BATCH_SIZE,\n",
    "                                                      num_workers = 0,\n",
    "                                                      shuffle = True)\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(dataset = dataset_test,\n",
    "                                                      batch_size = BATCH_SIZE,\n",
    "                                                      num_workers = 0,\n",
    "                                                      shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9c86c-7e5f-4745-b5e8-49e7c8009127",
   "metadata": {},
   "source": [
    "## Определение модели, оптимизатора и метода learning rate annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba6919f-6c96-421e-9cb8-142a2101e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_sil = VGG19(num_classes=1).to(device)\n",
    "descriptor_col = VGG19(num_classes=1).to(device)\n",
    "\n",
    "descriptor_sil.load_state_dict(torch.load(Path('./models') / 'descriptor_sil' / 'best.pt'))\n",
    "descriptor_col.load_state_dict(torch.load(Path('./models') / 'descriptor_col' / 'best.pt'))\n",
    "\n",
    "\n",
    "sil_optimizer = torch.optim.Adam(descriptor_sil.parameters(), lr=1e-5)\n",
    "col_optimizer = torch.optim.Adam(descriptor_col.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "model = MeshVAE().to(device)\n",
    "model.load_state_dict(torch.load(Path('./models') / 'MeshVAE' / '16' / 'last.pt'))\n",
    "criterion = VAELossGAN(device=device, clamp=True, clamp_threshold=10000., \n",
    "                       adv_net_sil=descriptor_sil, adv_net_sil_optim=sil_optimizer, \n",
    "                       adv_net_col=descriptor_col, adv_net_col_optim=col_optimizer,\n",
    "                       KLD_coef = 1.,)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = None # torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=60, T_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f9bf9-79e3-4692-996d-52743d4c777d",
   "metadata": {},
   "source": [
    "## Обучение!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf89256-b3fc-466a-ae33-4255ea34443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Epoch 1==========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/miniforge3/envs/pytorch3d/lib/python3.9/site-packages/torch/utils/checkpoint.py:553: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/student/miniforge3/envs/pytorch3d/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/student/miniforge3/envs/pytorch3d/lib/python3.9/site-packages/pytorch3d/renderer/implicit/raymarching.py:188: UserWarning: One or more elements of rays_densities are outside of validrange (0.0, 1.0)\n",
      "  warnings.warn(\n",
      "100%|██████████| 399/399 [30:22<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.90 | Test loss: 0.90\n",
      "Saved weights to models/MeshVAE/16/best.pt\n",
      "==========Epoch 2==========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [30:19<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.89 | Test loss: 0.89\n",
      "Saved weights to models/MeshVAE/16/best.pt\n",
      "==========Epoch 3==========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [30:18<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.89 | Test loss: 0.89\n",
      "Saved weights to models/MeshVAE/16/last.pt\n",
      "==========Epoch 4==========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [30:23<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.88 | Test loss: 0.88\n",
      "Saved weights to models/MeshVAE/16/best.pt\n",
      "==========Epoch 5==========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [30:20<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.89 | Test loss: 0.89\n",
      "Saved weights to models/MeshVAE/16/last.pt\n",
      "==========Epoch 6==========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [30:20<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.89 | Test loss: 0.89\n",
      "Saved weights to models/MeshVAE/16/last.pt\n",
      "==========Epoch 7==========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 19/399 [01:27<28:52,  4.56s/it]"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "torch.cuda.empty_cache()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "max_VRAM_optim_fit(train_launch=16,\n",
    "                   model=model, optimizer=optimizer, criterion=criterion,\n",
    "                   train_dl=train_dl, test_dl=test_dl, \n",
    "                   device=device, epochs=epochs,\n",
    "                   scheduler=scheduler,\n",
    "                   weights_save_dir=Path('./models'),\n",
    "                   early_stopping=False, early_stopping_tolerance = 5,\n",
    "                   restore_best_weights=False, save_loss_history=True, overfit_mode=False, mode='gan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "983a0150-e95a-47c0-93ca-906300c377e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Path('./models') / 'descriptor_sil').mkdir(exist_ok=True)\n",
    "(Path('./models') / 'descriptor_col').mkdir(exist_ok=True)\n",
    "torch.save(descriptor_sil.state_dict(), Path('./models') / 'descriptor_sil' / 'best.pt')\n",
    "torch.save(descriptor_col.state_dict(), Path('./models') / 'descriptor_col' / 'best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8262d142-cb52-4b06-8fb1-547b5879a941",
   "metadata": {},
   "source": [
    "## Инференс!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fd36de6-a78e-4a25-86fa-93f1c8ad164b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeshVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv3d(40, 64, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3))\n",
       "    (res_down_block1): ResDown(\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (act_fnc): ELU(alpha=1.0)\n",
       "    )\n",
       "    (res_down_block2): ResDown(\n",
       "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (act_fnc): ELU(alpha=1.0)\n",
       "    )\n",
       "    (res_down_block3): ResDown(\n",
       "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (act_fnc): ELU(alpha=1.0)\n",
       "    )\n",
       "    (res_down_block4): ResDown(\n",
       "      (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv3d(512, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv3d(512, 1024, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (act_fnc): ELU(alpha=1.0)\n",
       "    )\n",
       "    (act_fnc): ELU(alpha=1.0)\n",
       "  )\n",
       "  (reparametrize): Reparametrize(\n",
       "    (conv_mu): Conv2d(1024, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (conv_log_var): Conv2d(1024, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_t_up): ConvTranspose3d(512, 1024, kernel_size=(4, 4, 4), stride=(1, 1, 1))\n",
       "    (res_up_block1): ResUp(\n",
       "      (conv1): Conv3d(1024, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv3d(1024, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (up_nn): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (act_fnc): ELU(alpha=1.0)\n",
       "    )\n",
       "    (res_up_block2): ResUp(\n",
       "      (conv1): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (up_nn): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (act_fnc): ELU(alpha=1.0)\n",
       "    )\n",
       "    (res_up_block3): ResUp(\n",
       "      (conv1): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (up_nn): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (act_fnc): ELU(alpha=1.0)\n",
       "    )\n",
       "    (res_up_block4): ResUp(\n",
       "      (conv1): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (up_nn): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (act_fnc): ELU(alpha=1.0)\n",
       "    )\n",
       "    (conv_out): Conv3d(64, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (act_fnc): ELU(alpha=1.0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = MeshVAE().to(device)\n",
    "model.load_state_dict(torch.load(Path('./models') / 'MeshVAE' / '15' / 'last.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c50e6789-087b-4732-89a5-0c085705d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dataset, dataset_model_number):\n",
    "    get_renders = torch.stack((dataset[dataset_model_number], dataset[dataset_model_number]), 0)\n",
    "    # get_renders = dataset[dataset_model_number].unsqueeze(0)\n",
    "    print(get_renders.shape)\n",
    "\n",
    "    ren_imgs, rend_sils = None, None\n",
    "    target_imgs, target_sils = None, None\n",
    "    with torch.inference_mode():\n",
    "        # get model\n",
    "        input_renders = get_renders\n",
    "        target_renders, target_silhouttes = input_renders.to(device).split([1, 1], dim = 1)\n",
    "        target_renders, target_silhouttes = target_renders.squeeze().unsqueeze(2), target_silhouttes.squeeze().unsqueeze(2)\n",
    "        target_imgs, target_sils = target_renders.cpu(), target_silhouttes.cpu()\n",
    "        print(target_renders.shape)\n",
    "        mu, logvar, output_model = model(target_renders)\n",
    "    \n",
    "        num_views, azimuth_range = 40, 180.0\n",
    "        elev = torch.linspace(0, 0, num_views)  # keep constant\n",
    "        azim = torch.linspace(-azimuth_range, azimuth_range, num_views) + 180.0\n",
    "        R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
    "        target_cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "        target_cameras = target_cameras.to(device)\n",
    "    \n",
    "        rendered_images_silhouettes = model.get_renders(output_model, target_cameras)\n",
    "        rendered_images, rendered_silhouttes = rendered_images_silhouettes.split([1, 1], dim = -1)\n",
    "        ren_imgs, rend_sils = rendered_images.permute(0, 1, 4, 2, 3), rendered_silhouttes.permute(0, 1, 4, 2, 3)\n",
    "    \n",
    "        del input_renders\n",
    "        del rendered_images_silhouettes\n",
    "        del rendered_images\n",
    "        del rendered_silhouttes\n",
    "        del target_cameras\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    ren_imgs, rend_sils = ren_imgs.squeeze().detach().cpu()[0], rend_sils.squeeze().detach().cpu()[0]\n",
    "    # print(ren_imgs.shape, rend_sils.shape)\n",
    "    return ren_imgs, rend_sils, target_imgs, target_sils, mu.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be6e1156-f278-46f6-bd9a-14898fb76205",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_class = pd.read_csv('./modelnet_renders_metadata.csv')\n",
    "data_per_class = data_per_class.groupby('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "489eba03-07e7-4f8f-af89-2c9723c4073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 1, 40, 64, 64])\n",
      "torch.Size([2, 40, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "dataset_model_number = 2500\n",
    "datasets = [dataset_train, dataset_test]\n",
    "dataset = datasets[0]\n",
    "\n",
    "ren_imgs, rend_sils, target_imgs, target_sils, mu = get_model(dataset=dataset, dataset_model_number=dataset_model_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a98b76-3200-45f2-a1bb-a71cbb6b0d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3990/3990 [13:14<00:00,  5.02it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets[0]\n",
    "prev_latent = None\n",
    "mu_mean = torch.zeros(3990, 512)\n",
    "cum_diff = torch.zeros(512).float()\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    _, _, _, _, mu = get_model(dataset=dataset, dataset_model_number=i)\n",
    "    latent = mu[0].squeeze()\n",
    "    if prev_latent is None:\n",
    "        prev_latent = latent\n",
    "    else:\n",
    "        cum_diff += (latent - prev_latent).abs()\n",
    "        prev_latent = latent\n",
    "    mu_mean[i] = latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2c2df9a-1f97-43ac-9998-d509f9e82a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 86, 469, 186, 474, 210, 408, 292, 143, 305,  24])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset = torch.topk(cum_diff, 10).indices\n",
    "all_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "264a9573-6e9e-4c39-9d23-e9d338871d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 86, 469, 186, 474, 210, 408, 305, 292, 143,  24])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset2 = torch.topk(cum_diff, 10).indices\n",
    "all_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "770a2141-473c-435e-9e6b-447bfa1daaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_mean.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "437d4268-58ac-42c6-95c9-338a3bc8099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mu_mean.mean(axis=0), 'latent_mean.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c390201d-4d31-480c-821c-929372f7e10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffa2c434820>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEnCAYAAAAJnCGMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoVElEQVR4nO3df3BUVZr/8U/zq0lCEhSG7jQJbNC4gMCI4EajNWHUZItxnbXYckbxB1NbtQUCDhlqlx/DVk3GkoRhayh2ipVZqCmHKZfF2hqddbdmlLhqdJZlCWAUYUewiIJKzIAxHSQkQM73D4v+9j0JfXPTnZvu5P2q6qp++t6+ffqYvjye89xzA8YYIwAAAJ+MGOwGAACA4YXkAwAA+IrkAwAA+IrkAwAA+IrkAwAA+IrkAwAA+IrkAwAA+IrkAwAA+IrkAwAA+IrkAwAA+GrAko9nnnlGxcXFGjt2rObNm6e33nproD4KAABkkFEDcdDnn39eVVVVeuaZZ3TnnXfqn//5n7Vw4UIdO3ZMU6ZMSfje7u5uffrpp8rNzVUgEBiI5gEAgBQzxqi9vV2RSEQjRiQe2wgMxI3lSktLdeutt2r79u2x12bMmKEHHnhAtbW1Cd/78ccfq6ioKNVNAgAAPjh9+rQKCwsT7pPykY+uri4dOnRI69atc7xeWVmpffv29di/s7NTnZ2dsfhqLnT48GGNGzcu1c0DMMhuuummwW4CgAGUm5vruk/Kk4+zZ8/qypUrCoVCjtdDoZCam5t77F9bW6sf//jHPV4fN25cn74AAABIH30pmRiwglP7w40xvTZo/fr1amtriz1Onz49UE0C0EeBQOCaDwBIVspHPiZOnKiRI0f2GOVoaWnpMRoiScFgUMFgMNXNAAAAaSrlIx9jxozRvHnzVFdX53i9rq5OZWVlqf44AACQYQbkUtvVq1frscce0/z583XHHXdox44dOnXqlJYtWzYQHwcAADLIgCQf3/3ud3Xu3Dk99dRTOnPmjGbNmqXf/va3mjp1ap+PkWh+eQCuDgYAAD4ZkHU+khGNRpWfn68TJ05c82qXNGsyMOQkKixN9vdXUFCQ1PsBpLe2tjbl5eUl3Id7uwAAAF+RfAAAAF8NSM3HQOttDREA/cf6HQD8xMgHAADwFckHAADwFckHAADwVdrWfBhj+lzLMZCXBQLDgf07oa4KwEBi5AMAAPiK5AMAAPgqbaddvGBIGEgtflMABhIjHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFckHwAAwFeek48333xT999/vyKRiAKBgH7zm984thtjVF1drUgkoqysLC1YsEBHjx5NVXsBAECG85x8fPnll/r617+ubdu29bp98+bN2rJli7Zt26aGhgaFw2FVVFSovb096cYCAIDMN8rrGxYuXKiFCxf2us0Yo61bt2rDhg1atGiRJGnXrl0KhULavXu3li5dmlxrAQBAxktpzUdTU5Oam5tVWVkZey0YDKq8vFz79u3r9T2dnZ2KRqOOBwAAGLpSmnw0NzdLkkKhkOP1UCgU22arra1Vfn5+7FFUVJTKJgEAgDQzIFe7BAIBR2yM6fHaVevXr1dbW1vscfr06YFoEgAASBOeaz4SCYfDkr4aASkoKIi93tLS0mM05KpgMKhgMJjKZgAAgDSW0pGP4uJihcNh1dXVxV7r6upSfX29ysrKUvlRAAAgQ3ke+Th//rw++OCDWNzU1KTGxkZdf/31mjJliqqqqlRTU6OSkhKVlJSopqZG2dnZWrx4cUobDgAAMpPn5OPgwYP65je/GYtXr14tSVqyZIl++ctfas2aNero6NDy5cvV2tqq0tJS7d27V7m5ualrNQAAyFgBY4wZ7EbEi0ajys/P1/Hjx0lYgCEovh4MwNDT1tamvLy8hPtwbxcAAOArkg8AAOCrlF5qC8Af9ro5va2tkygGkF5GjPj/YwGjRjn/ab58+bIj7u7u9qVNA4mRDwAA4CuSDwAA4CumXYA0FT8MK0mjR4+OPbdXBR45cqQjvnDhgiPu7OxMcesApNKsWbNiz1etWuXYtn37dkd88OBBX9o0kBj5AAAAviL5AAAAviL5AAAAvqLmA/CJ2+Wxdt2GXdeRlZV1zX3tS2ntS/Go+QDS24QJE2LP7777bse2kydPOuLDhw874ky89JaRDwAA4CuSDwAA4CuSDwAA4CtqPoA4bnUZ8bG9Doddh+EW2+/3GifiZV8APSVaZ0eSxowZ44jz8/Mdsb1EemFhYcLjVVRUXPO98+fPd8Q5OTmOuL29XZmGMxQAAPAVyQcAAPAVyQcAAPAVNR/IOF7qLuzt9lyq17oMuwYk0bZkb3tvx14+2227fWwgXcT/5uy6CPv3eN111zli+/ddUFDgiLOzsx1xOBx2xJFIJPbcruG44YYbHLFdd2F/lt2W8ePHO2L7XBP/Xe3veeONNzriadOmOeJ33nlHmYaRDwAA4CuSDwAA4CuSDwAA4CtqPuA7u/7ArW7DnjuN39+thsNrHYZbHM+tbsKtDsNNMu+332v306VLl/p9bAxvbvVDdp1G/D1LJGnGjBmO+JZbbnHE8XUcdp2FvbbG5MmTE362Xbdhv9/tXJMurr/+ekd8++23O2JqPgAAAFyQfAAAAF+RfAAAAF+l5wQXBp1bvUF83UWya2XY86yJajzs9ydbV+FVojqPZGtAUrkWh30se32Dtra2fh8bmcf+Tdp/D3ZdxtSpU2PPZ86c6dhm36Pk7Nmzjnj27NmOeNasWY544sSJjjgrK+tazc5oyZyb7N++fQ687bbbHPEvf/lLR9zZ2dnvz/YLIx8AAMBXnpKP2tpa3XbbbcrNzdWkSZP0wAMP6P3333fsY4xRdXW1IpGIsrKytGDBAh09ejSljQYAAJnLU/JRX1+vFStWaP/+/aqrq9Ply5dVWVmpL7/8MrbP5s2btWXLFm3btk0NDQ0Kh8OqqKjIyFv+AgCA1AuYJCaW//jHP2rSpEmqr6/XN77xDRljFIlEVFVVpbVr10r6au4pFArpJz/5iZYuXep6zGg0qvz8fB0/fly5ubn9bdqQZM8hjh079pr7ut0XxOs9TxLVXbjdk8SNfWw77u7u7vN7U30PEz/X+XDrR/u7JuK2jse5c+f63rAUs++Bga/Y9ymZNGlS7PnXvvY1xza7ruLixYuO2F7vYvr06Y64pKTEEdvrZcS3xW6XfU+Trq4uJeJWf5Bou9ffkNv+Xu6XZHP7/Xk9tr398uXL1zyW7dSpU474wQcfTLjdb21tbcrLy0u4T1I1H1eL1q4ugNLU1KTm5mZVVlbG9gkGgyovL9e+ffuS+SgAADBE9PtqF2OMVq9erbvuuitWzdzc3CxJCoVCjn1DoZA++uijXo/T2dnpyHyj0Wh/mwQAADJAv0c+Vq5cqXfffVf/+q//2mNbb0Pf1xqCqq2tVX5+fuxRVFTU3yYBAIAM0K+RjyeffFIvvfSS3nzzTcc13+FwWNJXIyDx87otLS09RkOuWr9+vVavXh2Lo9EoCcg1uF2r74XXudH4+cje4kTvdZsLte+34OX+CqleO8PvdUP8MlS/Vyaz6zI2btzoiOOnr8ePH+/YZp8Lrly54og7OjocsVu9QqK/D7vmyj72H//4R0dsryFjH9uu8bDvvxL/Xd3OBfb3tr+n27nIjuP7Ndn6Mbvf3OrqvNxvyV4rZcqUKY54sGs++sLTyIcxRitXrtQLL7yg1157TcXFxY7txcXFCofDqquri73W1dWl+vp6lZWV9XrMYDCovLw8xwMAAAxdnkY+VqxYod27d+vf//3flZubG6vxyM/PV1ZWlgKBgKqqqlRTU6OSkhKVlJSopqZG2dnZWrx48YB8AQAAkFk8JR/bt2+XJC1YsMDx+rPPPqvvfe97kqQ1a9aoo6NDy5cvV2trq0pLS7V3714umwUAAJI8Jh99mfMKBAKqrq5WdXV1f9uEa0hUh2HPfbqx5yOTXasjEXvu1I7ttnup+QAylf0b/PDDDx3x66+/Hntu14dcuHDBEZ88edIRv/vuu47Yax2Wl3V8ro6AXysOBoMJ215RUeGI/+Ef/iH23K6L8FrD4bZWjv3fIP5c5PbZyUq0fpEbu23230cm4N4uAADAVyQfAADAV4xvZxB7eiL+kja3qQ17WM4eCk1mGWO3z/bK7RK3VA9/DkWp/m+C1LN/zz/96U8dcfx/M7fpgkSXvifL/tuxl1u3l8u3l3K3lwSwz0W33367I46fdvW6ZLnXy/rty1vj+9Xuc/u/l9fzlNttJIab4f3tAQCA70g+AACAr0g+AACAr6j5GCLc5hvt+cpkLvOy2Zd9ud3m3ma3hfqE5CVz63AMDq+Xy/vFvjXGz372M0c8Z84cRzx58mRHbC+nbn9P+/zhpX7F6+0V7HNNovOg3Y5k6uKknvUlw/03ycgHAADwFckHAADwFckHAADwFTUfw4Q9f3n+/HlH7Db/mKiOw63Gw+vcpr0GCbwb7vPJSJ1z58454o0bNzricePGOeKcnBxH/PnnnyeMS0tLHfGmTZtiz+11OdzONW5rZ9jnQbvmw64/SbSv2zof9vaBXq490zDyAQAAfEXyAQAAfEXyAQAAfEXNByS5zz8O5BoE8fdyAJBe7PUp3nnnnZQef+rUqY44/lzkdY0gN251F4k+2+uaIja7HsVui93PQx0jHwAAwFckHwAAwFckHwAAwFdMtmPQpdOaFEPlnihD5XsA8ZKtu3CrEUlm7Q2399o1HqNHj+73Zw0FjHwAAABfkXwAAABfkXwAAABfUfMBxEmn2ohk2uJ1bny432cCmcntXi82u+6iq6urz5/ltu6HXU9i/6bsdTzs+8wMN4x8AAAAX5F8AAAAX5F8AAAAX1HzAcRxWx/Dz9qIZNbqsN9rz41T44FMkOjeK73Fdk2Hzcu9YFK9Vo792elUXzYYGPkAAAC+8pR8bN++XXPmzFFeXp7y8vJ0xx136He/+11suzFG1dXVikQiysrK0oIFC3T06NGUNxoAAGQuT8lHYWGhNm3apIMHD+rgwYO6++679Zd/+ZexBGPz5s3asmWLtm3bpoaGBoXDYVVUVKi9vX1AGg8AADKPp+Tj/vvv17e+9S3ddNNNuummm7Rx40aNGzdO+/fvlzFGW7du1YYNG7Ro0SLNmjVLu3bt0oULF7R79+6Baj+APjDGOB6BQMDxADKR17/jESNGOB72++N/Izb7N+TWlpEjRzoeo0aNSvgYbvpd83HlyhXt2bNHX375pe644w41NTWpublZlZWVsX2CwaDKy8u1b9++ax6ns7NT0WjU8QAAAEOX5+TjyJEjGjdunILBoJYtW6YXX3xRM2fOVHNzsyQpFAo59g+FQrFtvamtrVV+fn7sUVRU5LVJAAAgg3hOPv70T/9UjY2N2r9/v5544gktWbJEx44di23v7dLERMNh69evV1tbW+xx+vRpr00CAAAZxPNE05gxY3TjjTdKkubPn6+Ghgb94z/+o9auXStJam5uVkFBQWz/lpaWHqMh8YLBoILBoNdmAP2S7Lod8e8f6LUyUlmLQV0HMpH9d2uv42HXStjr2XR2djpi+14u9tob8e+3j2V/ltuaIhcvXnTEbmuUDDdJr/NhjFFnZ6eKi4sVDodVV1cX29bV1aX6+nqVlZUl+zEAAGCI8DTy8cMf/lALFy5UUVGR2tvbtWfPHr3xxht6+eWXFQgEVFVVpZqaGpWUlKikpEQ1NTXKzs7W4sWLB6r9AAAgw3hKPj777DM99thjOnPmjPLz8zVnzhy9/PLLqqiokCStWbNGHR0dWr58uVpbW1VaWqq9e/cqNzd3QBqP4Sl+uNLrEsjDfagTyCRuv2976sO+bb3X33ui/e3PvnLliiO2p3DcbnFgt92eIhrqPCUfv/jFLxJuDwQCqq6uVnV1dTJtAgAAQxj3dgEAAL4i+QAAAL4afmu6Iu14vQw0fv/hdAlp/BzycPreGL7sv/PLly87Yrvuwt7fLU50m3t7m12T4XbZvluNiB0PN4x8AAAAX5F8AAAAX5F8AAAAX1HzgYyTzDofADKX2xLl9toZA7mkub1uh13D4XZuGu7nKkY+AACAr0g+AACAr0g+AACAr6j5ADzw894wydSzcA8bDAVua2nYkv27T7SWjltb7BqQVLdtqGHkAwAA+IrkAwAA+IrkAwAA+IqaDww6r3OrACAlv1ZGonOPXcNh3+vFjX3eGj16tCO2j2/fO2aoY+QDAAD4iuQDAAD4iuQDAAD4ipoPpJ2BvB8DgMzltcbDPneMHTvWEV+6dMkRx9+fJdl1POzY/iy34w11w/vbAwAA35F8AAAAX5F8AAAAX1HzgUHndv18onU/kr3Of6igLgYDxV6fYubMmY543LhxjjgnJ8cRf/755wnjcDjsiON/08nc36i3/bu6uhyxfe5J9DtyW/fD7Tw2cuTIhLHdtqGOkQ8AAOArkg8AAOArkg8AAOAraj4gqefcqD0fOWbMGEccP/85apTzz8iO7blSr/duSXT9fLJzwkPFcP3eGHgTJkxwxBs2bHDEc+bMccSTJ092xPY9S+LX0pB6nmsuX758zbZ4XXvD7byW6Nzidp7yeu8XrzUiQx0jHwAAwFdJJR+1tbUKBAKqqqqKvWaMUXV1tSKRiLKysrRgwQIdPXo02XYCAIAhot/JR0NDg3bs2NFjyG3z5s3asmWLtm3bpoaGBoXDYVVUVKi9vT3pxgIAgMzXr5qP8+fP65FHHtHOnTv19NNPx143xmjr1q3asGGDFi1aJEnatWuXQqGQdu/eraVLl6am1XBlz1cGg8GE2+3Yns9MFNvb7Dldew7Xbe7Uri9xayswFNj1CPF/5271B4nqJJL12WefOeJly5Y54oKCAkd83XXXOeLs7GxHbK8bcs899zjiBx98MPbcrh/zWi/mdf9E+7qdt9xqQOzz4nBfm6dfIx8rVqzQfffdp3vvvdfxelNTk5qbm1VZWRl7LRgMqry8XPv27ev1WJ2dnYpGo44HAAAYujyPfOzZs0eHDx9WQ0NDj23Nzc2SpFAo5Hg9FArpo48+6vV4tbW1+vGPf+y1GQAAIEN5Gvk4ffq0Vq1apeeee67HrYnj9TbUda2h8vXr16utrS32OH36tJcmAQCADONp5OPQoUNqaWnRvHnzYq9duXJFb775prZt26b3339f0lcjIPHzgC0tLT1GQ64KBoM96hHQOzuBi08A3dbSsHldW8Oer0zlNer2vK5bW5KRzBxwJhmq32sosWs8VqxY4Yjj76HiVld18uRJR/zuu+8m3N+tjip+f/uz7b+tTz/91BH/4Q9/cMT2+f3ChQuO2L4XzF/91V/pWrzWfyW7NoeXY3utP3E73lDnaeTjnnvu0ZEjR9TY2Bh7zJ8/X4888ogaGxs1bdo0hcNh1dXVxd7T1dWl+vp6lZWVpbzxAAAg83ga+cjNzdWsWbMcr+Xk5GjChAmx16uqqlRTU6OSkhKVlJSopqZG2dnZWrx4cepaDQAAMlbKl1dfs2aNOjo6tHz5crW2tqq0tFR79+5Vbm5uqj9qyLOHZe0+TDSMl+iyvf7EtkRLnLtxGwodyEtpk22rn1LZD1yenH7sv/spU6Y44vLy8thze9rafq+9hLnb7dndzg/x0xH2Z+Xl5TniM2fOOOLW1lZHbE+rXrx40RGPHz/eEcdfam+/d6Bvp5DoeF7PBV6XM/AyBWRPo126dMlT29JB0snHG2+84YgDgYCqq6tVXV2d7KEBAMAQxL1dAACAr0g+AACAr1Je84HUsecAz58/74jj50O9LIfeWzyQ9QVusddL1BItO+21bcnIpEvjMqmtw4U9Tx9/qwpJ+vnPfx57XlhY6Ng2Y8YMR2yfG+wlzadPn+6Ib7zxRkdsHz9+ifRx48YlPHZRUZEjtutT3JYZt+tVEu2brGSOZ58zvXJbviD+3OT2ez179qwjPnXqVFJtGwyMfAAAAF+RfAAAAF+RfAAAAF9R85HG3G6bner50EQS1Uq41VHYc6X2GgP2tfz2Lbe9rGeS7Lysbaguxz5UvsdQ8sUXX1wz/uCDDxzb7CUO3Ni/E3tJ84kTJzri+DqO2bNnO7bZNR2ff/65I7755psd8dy5cx1xfD2JJGVlZV2r2Wn1d+r1fDuQa+scOXLEEX/22WcD9lkDhZEPAADgK5IPAADgK5IPAADgK2o+Mpif86HJfJbbfQjc1gGxa0LiY3ubWw1Isve08cLtPhR+/vezb2OO4cX+DUaj0YTxyZMnY8/r6+sd29z+juPvzSL1rPGw1yhJVBNyww03JDz25MmTHbFdL5afn5/w/W71Z8lI5e/bXiuloaHBESdaKyVdMfIBAAB8RfIBAAB8RfIBAAB8Rc0HBp09N2rHXV1d14zt+We3e9jYc7xuc76Jakj8rB/xyu5Daj6QKm61DPbv1V6Dwo7tNUvif2N2DYf9e7brSezfb0FBgSO270sTDocdcSQSiT2360Xs+hN7rRT7s+y2jB8/3hHb55b472p/T3stlf379yvTMfIBAAB8RfIBAAB8RfIBAAB8Rc0HMpo9/2yvZ+C2xojNrW4jfp7WbU0Rr2uQeF2TJJ7bPLy9TgCQruL/Vt3Wr+jo6Ei4/dSpU/1uh/17tOtP7DVD7BoR+/dfWFiY8HgVFRWx54899phjm31vn/h1WDIVIx8AAMBXJB8AAMBXJB8AAMBX1HwAcdzWHImfj/ZaP+K23Z5jHjdunCOOnyO260cGc00RYCiy66Ts+hM7bm9vT3i8Dz/8sM+f/fDDDzvigwcPOuIvv/yyz8dKV4x8AAAAX5F8AAAAX5F8AAAAX1HzAQwQt7U3EtWTSFJbW5sjzsvLiz0fO3ZswmNfvny5L00EkCbOnTsXe/7aa685tr3yyiuOeCis28PIBwAA8JWn5KO6ulqBQMDxiL8roDFG1dXVikQiysrK0oIFC3T06NGUNxoAAGQuzyMfN998s86cORN7HDlyJLZt8+bN2rJli7Zt26aGhgaFw2FVVFS4XoIEAACGD881H6NGjXKMdlxljNHWrVu1YcMGLVq0SJK0a9cuhUIh7d69W0uXLk2+tcAwYteExF/bb9+zxr7PRFdX18A1DEDKvffee7HnTzzxhGPbUKzh8jzyceLECUUiERUXF+uhhx6K3eCmqalJzc3NqqysjO0bDAZVXl6uffv2XfN4nZ2dikajjgcAABi6PCUfpaWl+tWvfqVXXnlFO3fuVHNzs8rKynTu3Dk1NzdLkkKhkOM9oVAotq03tbW1ys/Pjz2Kior68TUAAECm8DTtsnDhwtjz2bNn64477tANN9ygXbt26fbbb5fUc5lnY0zCpZ/Xr1+v1atXx+JoNEoCAvQifujVXl55KCy3DAxn8ZfPDodp06Qutc3JydHs2bN14sSJWB2IPcrR0tLSYzQkXjAYVF5enuMBAACGrqSSj87OTv3f//2fCgoKVFxcrHA4rLq6utj2rq4u1dfXq6ysLOmGAgCAocHTtMvf/u3f6v7779eUKVPU0tKip59+WtFoVEuWLFEgEFBVVZVqampUUlKikpIS1dTUKDs7W4sXLx6o9gMAgAzjKfn4+OOP9fDDD+vs2bP62te+pttvv1379+/X1KlTJUlr1qxRR0eHli9frtbWVpWWlmrv3r3Kzc0dkMYDw5Xb0u0AkM4CJs3OYtFoVPn5+Tp+/DhJCzAEFRQUDHYTAAygtrY21/pN7u0CAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB8RfIBAAB85Tn5+OSTT/Too49qwoQJys7O1i233KJDhw7FthtjVF1drUgkoqysLC1YsEBHjx5NaaMBAEDm8pR8tLa26s4779To0aP1u9/9TseOHdNPf/pTjR8/PrbP5s2btWXLFm3btk0NDQ0Kh8OqqKhQe3t7qtsOAAAyUMAYY/q687p16/Tf//3feuutt3rdboxRJBJRVVWV1q5dK0nq7OxUKBTST37yEy1dutT1M6LRqPLz83X8+HHl5ub2tWkAMkRBQcFgNwHAAGpra1NeXl7CfTyNfLz00kuaP3++HnzwQU2aNElz587Vzp07Y9ubmprU3NysysrK2GvBYFDl5eXat29fr8fs7OxUNBp1PAAAwNDlKfk4efKktm/frpKSEr3yyitatmyZvv/97+tXv/qVJKm5uVmSFAqFHO8LhUKxbbba2lrl5+fHHkVFRf35HgAAIEN4Sj66u7t16623qqamRnPnztXSpUv1N3/zN9q+fbtjv0Ag4IiNMT1eu2r9+vVqa2uLPU6fPu3xKwAAgEziKfkoKCjQzJkzHa/NmDFDp06dkiSFw2FJ6jHK0dLS0mM05KpgMKi8vDzHAwAADF2eko8777xT77//vuO148ePa+rUqZKk4uJihcNh1dXVxbZ3dXWpvr5eZWVlKWguAADIdKO87PyDH/xAZWVlqqmp0Xe+8x0dOHBAO3bs0I4dOyR9Nd1SVVWlmpoalZSUqKSkRDU1NcrOztbixYsH5AsAAIDM4in5uO222/Tiiy9q/fr1euqpp1RcXKytW7fqkUceie2zZs0adXR0aPny5WptbVVpaan27t3LZbMAAECSx3U+/MA6H8DQxjofwNCW8nU+AAAAkkXyAQAAfEXyAQAAfEXyAQAAfEXyAQAAfOXpUls/XL345vz584PcEgAA4FVfLqJNu+Sjvb1dknTrrbcOcksAAIBX7e3tys/PT7hP2q3z0d3drU8//VTGGE2ZMkWnT5/mfi8eRKNRFRUV0W8e0Gf9Q795R5/1D/3m3WD0mTFG7e3tikQiGjEicVVH2o18jBgxQoWFhYpGo5LEzeb6iX7zjj7rH/rNO/qsf+g37/zuM7cRj6soOAUAAL4i+QAAAL5K2+QjGAzqRz/6kYLB4GA3JaPQb97RZ/1Dv3lHn/UP/eZduvdZ2hWcAgCAoS1tRz4AAMDQRPIBAAB8RfIBAAB8RfIBAAB8lbbJxzPPPKPi4mKNHTtW8+bN01tvvTXYTUobtbW1uu2225Sbm6tJkybpgQce0Pvvv+/Yxxij6upqRSIRZWVlacGCBTp69OggtTj91NbWKhAIqKqqKvYafda7Tz75RI8++qgmTJig7Oxs3XLLLTp06FBsO/3W0+XLl/X3f//3Ki4uVlZWlqZNm6annnpK3d3dsX2Ge7+9+eabuv/++xWJRBQIBPSb3/zGsb0v/dPZ2aknn3xSEydOVE5Ojr797W/r448/9vFb+C9Rv126dElr167V7NmzlZOTo0gkoscff1yffvqp4xhp0W8mDe3Zs8eMHj3a7Ny50xw7dsysWrXK5OTkmI8++miwm5YW/vzP/9w8++yz5r333jONjY3mvvvuM1OmTDHnz5+P7bNp0yaTm5trfv3rX5sjR46Y7373u6agoMBEo9FBbHl6OHDggPmTP/kTM2fOHLNq1arY6/RZT59//rmZOnWq+d73vmf+93//1zQ1NZlXX33VfPDBB7F96Leenn76aTNhwgTzn//5n6apqcn827/9mxk3bpzZunVrbJ/h3m+//e1vzYYNG8yvf/1rI8m8+OKLju196Z9ly5aZyZMnm7q6OnP48GHzzW9+03z96183ly9f9vnb+CdRv33xxRfm3nvvNc8//7z5wx/+YP7nf/7HlJaWmnnz5jmOkQ79lpbJx5/92Z+ZZcuWOV6bPn26Wbdu3SC1KL21tLQYSaa+vt4YY0x3d7cJh8Nm06ZNsX0uXrxo8vPzzc9//vPBamZaaG9vNyUlJaaurs6Ul5fHkg/6rHdr1641d9111zW302+9u++++8xf//VfO15btGiRefTRR40x9JvN/ke0L/3zxRdfmNGjR5s9e/bE9vnkk0/MiBEjzMsvv+xb2wdTb0mb7cCBA0ZS7H/e06Xf0m7apaurS4cOHVJlZaXj9crKSu3bt2+QWpXe2traJEnXX3+9JKmpqUnNzc2OPgwGgyovLx/2fbhixQrdd999uvfeex2v02e9e+mllzR//nw9+OCDmjRpkubOnaudO3fGttNvvbvrrrv0X//1Xzp+/Lgk6Z133tHvf/97fetb35JEv7npS/8cOnRIly5dcuwTiUQ0a9Ys+jBOW1ubAoGAxo8fLyl9+i3tbix39uxZXblyRaFQyPF6KBRSc3PzILUqfRljtHr1at11112aNWuWJMX6qbc+/Oijj3xvY7rYs2ePDh8+rIaGhh7b6LPenTx5Utu3b9fq1av1wx/+UAcOHND3v/99BYNBPf744/TbNaxdu1ZtbW2aPn26Ro4cqStXrmjjxo16+OGHJfH35qYv/dPc3KwxY8bouuuu67EP/1Z85eLFi1q3bp0WL14cu7lcuvRb2iUfVwUCAUdsjOnxGqSVK1fq3Xff1e9///se2+jD/+/06dNatWqV9u7dq7Fjx15zP/rMqbu7W/Pnz1dNTY0kae7cuTp69Ki2b9+uxx9/PLYf/eb0/PPP67nnntPu3bt18803q7GxUVVVVYpEIlqyZElsP/otsf70D334lUuXLumhhx5Sd3e3nnnmGdf9/e63tJt2mThxokaOHNkjA2tpaemRBQ93Tz75pF566SW9/vrrKiwsjL0eDocliT6Mc+jQIbW0tGjevHkaNWqURo0apfr6ev3sZz/TqFGjYv1CnzkVFBRo5syZjtdmzJihU6dOSeJv7Vr+7u/+TuvWrdNDDz2k2bNn67HHHtMPfvAD1dbWSqLf3PSlf8LhsLq6utTa2nrNfYarS5cu6Tvf+Y6amppUV1cXG/WQ0qff0i75GDNmjObNm6e6ujrH63V1dSorKxukVqUXY4xWrlypF154Qa+99pqKi4sd24uLixUOhx192NXVpfr6+mHbh/fcc4+OHDmixsbG2GP+/Pl65JFH1NjYqGnTptFnvbjzzjt7XMZ9/PhxTZ06VRJ/a9dy4cIFjRjhPL2OHDkydqkt/ZZYX/pn3rx5Gj16tGOfM2fO6L333hvWfXg18Thx4oReffVVTZgwwbE9bfrNt9JWD65eavuLX/zCHDt2zFRVVZmcnBzz4YcfDnbT0sITTzxh8vPzzRtvvGHOnDkTe1y4cCG2z6ZNm0x+fr554YUXzJEjR8zDDz88rC7j64v4q12Moc96c+DAATNq1CizceNGc+LECfMv//IvJjs72zz33HOxfei3npYsWWImT54cu9T2hRdeMBMnTjRr1qyJ7TPc+629vd28/fbb5u233zaSzJYtW8zbb78duyqjL/2zbNkyU1hYaF599VVz+PBhc/fddw/5S20T9dulS5fMt7/9bVNYWGgaGxsd/z50dnbGjpEO/ZaWyYcxxvzTP/2TmTp1qhkzZoy59dZbY5eR4qvLq3p7PPvss7F9uru7zY9+9CMTDodNMBg03/jGN8yRI0cGr9FpyE4+6LPe/cd//IeZNWuWCQaDZvr06WbHjh2O7fRbT9Fo1KxatcpMmTLFjB071kybNs1s2LDB8Q/AcO+3119/vdfz2JIlS4wxfeufjo4Os3LlSnP99debrKws8xd/8Rfm1KlTg/Bt/JOo35qamq7578Prr78eO0Y69FvAGGP8G2cBAADDXdrVfAAAgKGN5AMAAPiK5AMAAPiK5AMAAPiK5AMAAPiK5AMAAPiK5AMAAPiK5AMAAPiK5AMAAPiK5AMAAPiK5AMAAPiK5AMAAPjq/wFBGXPZXz+ZsQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camera_num = 32\n",
    "targ_cam = target_imgs[0][camera_num].unsqueeze(0).squeeze()\n",
    "plt.imshow(torch.hstack((ren_imgs[camera_num] / torch.max(ren_imgs[camera_num]), targ_cam / torch.max(targ_cam))), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f354f8e2-3636-4d05-a183-5e7e1562dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imageio\n",
    "# from skimage import img_as_ubyte\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "imgs = list()\n",
    "save_path = Path('./recreated_models')\n",
    "save_path.mkdir(exist_ok=True)\n",
    "for camera_num, img in enumerate(ren_imgs):\n",
    "    targ_cam = target_imgs[0][camera_num].unsqueeze(0).squeeze()\n",
    "    imgs.append(to_pil_image(torch.hstack((ren_imgs[camera_num] / torch.max(ren_imgs[camera_num]), targ_cam)), mode='L'))\n",
    "    \n",
    "# imageio.mimsave('model_recreated16.gif', imgs)\n",
    "imgs[0].save(save_path / \"GAN_not_last_table.gif\", save_all=True, append_images=imgs[1:], duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee0b08-34ef-4a3b-a56a-1e7a41040d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch3D",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
